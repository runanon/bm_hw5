---
title: "biostatistical methods homework 5"
output: github_document
---


```{r message=FALSE}
library(knitr)
library(tidyverse)
library(faraway)
library(broom)
library(leaps)
library(boot)
library(modelr)
library(caret)
```

## R dataset ‘state.x77’ from library(faraway) contains information on 50 states from 1970s collected by US Census Bureau. The goal is to predict ‘life expectancy’ using a combination of remaining variables.

```{r}
life_data = as.data.frame(state.x77) %>%
  janitor::clean_names()
```

## 1. Explore the dataset and generate appropriate descriptive statistics and relevant graphs

```{r}
mean_and_sd = function(x) {
  
  if (!is.numeric(x)) {
    stop("Argument x should be numeric")
  } else if (length(x) == 1) {
    stop("Cannot be computed for length 1 vectors")
  }
  
  mean_x = mean(x)
  sd_x = sd(x)
  tibble(
    mean = mean_x, 
    sd = sd_x
  )
}
```

```{r message=FALSE}
attach(life_data)
```

```{r}
par(mfrow = c(2, 4))
boxplot(population, main = 'population')
boxplot(income,main = 'income' )
boxplot(illiteracy, main = 'illiteracy')
boxplot(life_exp, main = 'life_exp')
boxplot(murder, main = 'murder')
boxplot(hs_grad, main = 'hs_grad')
boxplot(frost, main = 'frost')
boxplot(area, main = 'area')
```

Population
```{r}
summary(population)
```

Income
```{r}
summary(income)
```

Illiteracy
```{r}
summary(illiteracy)
```

Life Exp
```{r}
summary(life_exp)
```

Murder
```{r}
summary(murder)
```

HS Grad
```{r}
summary(hs_grad)
```

Frost
```{r}
summary(frost)
```

Area
```{r}
summary(area)
```


## 2. Use automatic procedures to find a ‘best subset’ of the full model. Present the results and comment on the following:

```{r}
backward_fit <- lm(life_exp ~ ., data=life_data)
step(backward_fit, direction='backward') %>%
  summary()

forward_fit <- lm(life_exp ~ ., data=life_data)
step(forward_fit, direction='forward') %>%
  summary()

stepwise_fit <- lm(life_exp ~ ., data=life_data)
step(stepwise_fit, direction='both') %>%
  summary()
```

### a) Do the procedures generate the same model?

No. Using backward elimination, the model we obtained is: life_exp ~ population + murder + hs_grad + frost. Using forward elimination, the model we obtained is: life_exp ~ population + income + illiteracy + murder + hs_grad + frost + area. Using stepwise regression, the model we obtained is: life_exp ~ population + murder + hs_grad + frost.

### b) Is there any variable a close call? What was your decision: keep or discard? Provide arguments for your choice. (Note: this question might have more or less relevance depending on the ‘subset’ you choose).

Using backward elimination or stepwise regression, `population` is a close call variable with p-value of 0.05201.

```{r}
bw_s = lm(life_exp ~ murder + hs_grad + frost, data = life_data)
bw_l = lm(life_exp ~ murder + hs_grad + frost + population, data = life_data)
summary(bw_s)
summary(bw_l)
```

Judging from the Adjusted R-square, the differences between two models are less than 6%. So according to the principle of parsimony, I choose to discard `population`.

```{r}
backward_fit = lm(life_exp ~ murder + hs_grad + frost, data = life_data)
```

### c) Is there any association between ‘Illiteracy’ and ‘HS graduation rate’? Does your ‘subset’
contain both?

```{r}
cor.test(illiteracy, hs_grad, method="pearson")
```

Yes, there is association between `illiteracy` and `hs_grad`. The subset we got from forward elimination contains both.

## 3. Use criterion-based procedures studied in class to guide your selection of the ‘best subset’. Summarize your results (tabular or graphical).

```{r}
life_data = life_data %>%
  select(life_exp, everything())
```

```{r}
# Printing the 1 best models of each size, using the Cp criterion:
leaps(x = life_data[,2:8], y = life_data[,1], nbest=1, method="Cp")


# Printing the 1 best models of each size, using the adjusted R^2 criterion:
leaps(x = life_data[,2:8], y = life_data[,1], nbest=1, method="adjr2")

# Summary of models for each size (one model per size)
b<-regsubsets(life_exp ~ ., data=life_data)
   (rs<-summary(b))

# Plots of Cp and Adj-R2 as functions of parameters
par(mar=c(4,4,1,1))
par(mfrow=c(1,2))

plot(2:8, rs$cp, xlab="No of parameters", ylab="Cp Statistic")
abline(0,1)

plot(2:8, rs$adjr2, xlab="No of parameters", ylab="Adj R2")
```

Judging from the Cp statistics and Adjusted R-square, models with 4~8 parameters are better. 

```{r}
# AIC of the 3-predictor model:
pre_3 <- lm(life_exp ~ murder + hs_grad + frost, data = life_data)
AIC(pre_3)

# BIC
AIC(pre_3, k = log(length(life_exp)))

# AIC of the 4-predictor model:
pre_4 <- lm(life_exp ~ murder + hs_grad + frost + population, data = life_data)
AIC(pre_4)

# BIC
AIC(pre_4, k = log(length(life_exp)))

# AIC of the 5-predictor model:
pre_5 <- lm(life_exp ~ murder + hs_grad + frost + population + income, data = life_data)
AIC(pre_5)

# BIC
AIC(pre_5, k = log(length(life_data$life_exp)))

# AIC of the 6-predictor model:
pre_6 <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy, data = life_data)
AIC(pre_6)

# BIC
AIC(pre_6, k = log(length(life_data$life_exp)))

# AIC of the 7-predictor model:
pre_7 <- lm(life_exp ~ murder + hs_grad + frost + population + income + illiteracy + area, data = life_data)
AIC(pre_7)

# BIC
AIC(pre_7, k = log(length(life_data$life_exp)))

```

No of parameter   |4            |5            |6            |7            |8
------------------|-------------|-------------|-------------|-------------|-------------
Adjusted R-square |0.6939230    |0.7125690    |0.7061129    |0.6993268    |0.6921823
Cp                |3.7399       |2.0197       |4.0087       |6.0020       |8.0000
AIC               |117.974      |115.733      |117.720      |119.712      |121.709
BIC               |127.534      |127.205      |131.104      |135.008      |138.917

The model with 5 parameters (4 predictors) has the highest Adjusted R-square and lowest AIC and BIC. So the best model is the one with 5 parameters.

## 4. Compare the two ‘subsets’ from parts 2 and 3 and recommend a ‘final’ model. Using this ‘final’ model do the following:

Comparing model with 3 perdictors with model with 4 perdictors, since the differences between Adjusted R-square, AIC and BIC are pretty small, according to the principle of parsimony, I choose model with 3 perdictors, which is life_exp ~ murder + hs_grad + frost.

### a) Identify any leverage and/or influential points and take appropriate measures.

```{r}
par(mfrow=c(2,2))
plot(pre_3)
```

According to the Residuals vs Leverage plot, there is no leverage or influential points. 

### b) Check the model assumptions.

Judging from the QQ plot, the residuals are almost normally distributed. Judging from the Residuals vs Fitted values plot and Scale-Location plot, the residuals have constant variance. There is no certain pattern in Residuals vs Fitted values plot, so the residuals are independent.

## 5. Using the ‘final’ model chosen in part 4, focus on MSE to test the model predictive ability:

### a) Use a 10-fold cross-validation (10 repeats).

```{r}
set.seed(1)
data_train<-trainControl(method="cv", number=10)

model_caret<-train(life_exp ~ murder + hs_grad + frost,
                   data=life_data,
                   trControl=data_train,
                   method='lm',
                   na.action=na.pass)
model_caret
```

The RMSE is 0.759794, so the MSE is 0.5772869.

### b) Experiment a new, but simple bootstrap technique called “residual sampling”.

```{r}
boot_res = lm(life_exp ~ murder + hs_grad + frost, data=life_data)

pred = predict(boot_res)
resid = residuals(boot_res) 


res_data = tibble(resid = residuals(boot_res))

```


```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}

mse <- function(sm) 
    mean(sm$residuals^2)
```

Repeat 10 times

```{r}
set.seed(1)

list = ls()
i = 1
  
for (i in 1:10){
  res_boot = boot_sample(res_data)
  y_star = res_boot$resid + pred
  life_boot_data = bind_cols(life_data, tibble(y_star))
  boot_res_reg = lm(y_star ~ murder + hs_grad + frost, data=life_boot_data)
  list[i] = (mse(summary(boot_res_reg)))
  i = i + 1
}

repeat_10 = tibble(mse = list[1:10])
repeat_10

repeat_10 %>%
  mutate(mse = as.numeric(mse)) %>%
  summary()
```

Repeat 1000 times

```{r}
set.seed(1)

list = ls()
i = 1
  
for (i in 1:1000){
  res_boot = boot_sample(res_data)
  y_star = res_boot$resid + pred
  life_boot_data = bind_cols(life_data, tibble(y_star))
  boot_res_reg = lm(y_star ~ murder + hs_grad + frost, data=life_boot_data)
  list[i] = (mse(summary(boot_res_reg)))
  i = i + 1
}

repeat_1000 = tibble(mse = list[1:1000])
repeat_1000

repeat_1000 %>%
  mutate(mse = as.numeric(mse)) %>%
  summary()
```

### c) In a paragraph, compare the MSE values generated by the two methods a) and b). Briefly comment on the differences and your recommendation for assessing model performance.

Comparing MSEs generating from different methods, we can see that the MSEs of CV are higer than the MSE of residual sampling. The MSE of 1000 repeat residual sampling is lower than 10 repeat.
